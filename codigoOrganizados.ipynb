{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import SnowballStemmer \n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Inicializar stemmer y definir stopwords\n",
    "stemmer = SnowballStemmer('english')\n",
    "stopwords_path = r'C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\stopwords.txt'\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '',text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Leer y preprocesar documentos\n",
    "data_path = r'C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\training_txt'\n",
    "documents = []\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        path = os.path.join(data_path, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            documents.append((filename, preprocess_text(content)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de términos y documentos TF-IDF:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Vocabulario TF-IDF:\n",
      "[('bahia', 1187), ('cocoa', 2957), ('review', 17009), ('shower', 18020), ('continu', 3314), ('week', 20869), ('zone', 21400), ('allevi', 447), ('drought', 4568), ('earli', 4688), ('januari', 8099), ('improv', 7567), ('prospect', 16185), ('come', 3048), ('temporao', 19400), ('normal', 14488), ('humid', 7353), ('level', 8903), ('restor', 16954), ('comissaria', 3059)]\n",
      "Matriz de términos y documentos BoW:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Vocabulario BoW:\n",
      "[('bahia', 1187), ('cocoa', 2957), ('review', 17009), ('shower', 18020), ('continu', 3314), ('week', 20869), ('zone', 21400), ('allevi', 447), ('drought', 4568), ('earli', 4688), ('januari', 8099), ('improv', 7567), ('prospect', 16185), ('come', 3048), ('temporao', 19400), ('normal', 14488), ('humid', 7353), ('level', 8903), ('restor', 16954), ('comissaria', 3059)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Extraer contenidos preprocesados\n",
    "preprocessed_contents = [content for _, content in documents]\n",
    "\n",
    "# TF-IDF Vectorización\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(preprocessed_contents)\n",
    "print(\"Matriz de términos y documentos TF-IDF:\")\n",
    "print(X_tfidf.toarray()[:5])  # Mostrar primeras 5 filas\n",
    "print(\"Vocabulario TF-IDF:\")\n",
    "print(list(vectorizer_tfidf.vocabulary_.items())[:20])  # Primeros 20 términos\n",
    "\n",
    "# BoW Vectorización\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(preprocessed_contents)\n",
    "print(\"Matriz de términos y documentos BoW:\")\n",
    "print(X_bow.toarray()[:5])  # Mostrar primeras 5 filas\n",
    "print(\"Vocabulario BoW:\")\n",
    "print(list(vectorizer_bow.vocabulary_.items())[:20])  # Primeros 20 términos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del Índice Invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de términos en el índice invertido: 21411\n",
      "Índice Invertido (primeros 20 términos):\n",
      "       bahia   cocoa  review  shower  continu    week    zone  allevi  \\\n",
      "0        0.0     0.0     0.0     0.0      0.0     0.0     0.0     0.0   \n",
      "1     2047.0  3457.0  4098.0  5288.0      1.0  4096.0  2948.0   714.0   \n",
      "2      941.0  4226.0  7694.0  5292.0   2049.0     2.0  3077.0  3084.0   \n",
      "3     1240.0  2947.0  4625.0  5271.0   6146.0  2059.0   134.0  5037.0   \n",
      "4        NaN  5505.0    25.0   248.0   4109.0  6160.0   136.0  2417.0   \n",
      "...      ...     ...     ...     ...      ...     ...     ...     ...   \n",
      "3430     NaN     NaN     NaN     NaN      NaN     NaN     NaN     NaN   \n",
      "3431     NaN     NaN     NaN     NaN      NaN     NaN     NaN     NaN   \n",
      "3432     NaN     NaN     NaN     NaN      NaN     NaN     NaN     NaN   \n",
      "3433     NaN     NaN     NaN     NaN      NaN     NaN     NaN     NaN   \n",
      "3434     NaN     NaN     NaN     NaN      NaN     NaN     NaN     NaN   \n",
      "\n",
      "      drought   earli  januari  improv  prospect    come  temporao  normal  \\\n",
      "0         0.0     0.0      0.0     0.0       0.0     0.0       0.0     0.0   \n",
      "1      4226.0     4.0      2.0     1.0    3584.0  7171.0    4226.0  5650.0   \n",
      "2      2819.0  2059.0   2050.0  6146.0    3081.0  5131.0     941.0  6162.0   \n",
      "3      1924.0  6160.0   2053.0  2064.0    4114.0  3599.0    2047.0  6189.0   \n",
      "4       775.0  2065.0   4108.0  6167.0    2580.0  3088.0       NaN  2102.0   \n",
      "...       ...     ...      ...     ...       ...     ...       ...     ...   \n",
      "3430      NaN     NaN      NaN     NaN       NaN     NaN       NaN     NaN   \n",
      "3431      NaN     NaN      NaN     NaN       NaN     NaN       NaN     NaN   \n",
      "3432      NaN     NaN      NaN     NaN       NaN     NaN       NaN     NaN   \n",
      "3433      NaN     NaN      NaN     NaN       NaN     NaN       NaN     NaN   \n",
      "3434      NaN     NaN      NaN     NaN       NaN     NaN       NaN     NaN   \n",
      "\n",
      "       humid   level  restor  comissaria  \n",
      "0        0.0     0.0     0.0         0.0  \n",
      "1     5248.0  2059.0  6403.0         NaN  \n",
      "2     4226.0  4109.0  2573.0         NaN  \n",
      "3     5703.0  4110.0  1171.0         NaN  \n",
      "4     2104.0  4114.0  5268.0         NaN  \n",
      "...      ...     ...     ...         ...  \n",
      "3430     NaN     NaN     NaN         NaN  \n",
      "3431     NaN     NaN     NaN         NaN  \n",
      "3432     NaN     NaN     NaN         NaN  \n",
      "3433     NaN     NaN     NaN         NaN  \n",
      "3434     NaN     NaN     NaN         NaN  \n",
      "\n",
      "[3435 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "def build_inverted_index(preprocessed_contents):\n",
    "    inverted_index = {}\n",
    "    for doc_id, content in enumerate(preprocessed_contents):\n",
    "        for word in content.split():\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = set()\n",
    "            inverted_index[word].add(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Crear el índice invertido\n",
    "inverted_index = build_inverted_index(preprocessed_contents)\n",
    "print(f\"Total de términos en el índice invertido: {len(inverted_index)}\")\n",
    "index_df = pd.DataFrame.from_dict({k: list(v) for k, v in inverted_index.items()}, orient='index').transpose()\n",
    "print(\"Índice Invertido (primeros 20 términos):\")\n",
    "print(index_df.iloc[:, :20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del Motor de Búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documentos relevantes utilizando BoW:\n",
      "Documento 318, Similaridad: 0.33333333333333337\n",
      "Documento 7736, Similaridad: 0.2948839123097943\n",
      "Documento 319, Similaridad: 0.2886751345948129\n",
      "Documento 4708, Similaridad: 0.28530555551325537\n",
      "Documento 366, Similaridad: 0.2844400619942872\n",
      "\n",
      "Documentos relevantes utilizando TF-IDF:\n",
      "Documento 0, Similaridad: 0.36808505309292017\n",
      "Documento 318, Similaridad: 0.33691516276347605\n",
      "Documento 4708, Similaridad: 0.2936915872996356\n",
      "Documento 366, Similaridad: 0.29140135493961683\n",
      "Documento 319, Similaridad: 0.2908209663059065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def search_bow(query, vectorizer_bow, X_bow, inverted_index, preprocessor):\n",
    "    query = preprocessor(query)\n",
    "    query_words = query.split()\n",
    "    relevant_docs_set = set()\n",
    "    for word in query_words:\n",
    "        if word in inverted_index:\n",
    "            relevant_docs_set.update(inverted_index[word])\n",
    "    relevant_docs = list(relevant_docs_set)\n",
    "    if not relevant_docs:\n",
    "        return [], []\n",
    "    X_bow_relevant = X_bow[relevant_docs]\n",
    "    query_vector = vectorizer_bow.transform([query]).toarray()\n",
    "    similarities = cosine_similarity(query_vector, X_bow_relevant.toarray()).flatten()\n",
    "    sorted_docs = [relevant_docs[i] for i in np.argsort(similarities)[::-1]]\n",
    "    sorted_similarities = np.sort(similarities)[::-1]\n",
    "    return sorted_docs, sorted_similarities\n",
    "\n",
    "def search_tfidf(query, vectorizer_tfidf, X_tfidf, inverted_index, preprocessor):\n",
    "    query = preprocessor(query)\n",
    "    query_words = query.split()\n",
    "    relevant_docs_set = set()\n",
    "    for word in query_words:\n",
    "        if word in inverted_index:\n",
    "            relevant_docs_set.update(inverted_index[word])\n",
    "    relevant_docs = list(relevant_docs_set)\n",
    "    if not relevant_docs:\n",
    "        return [], []\n",
    "    X_tfidf_relevant = X_tfidf[relevant_docs]\n",
    "    query_vector = vectorizer_tfidf.transform([query]).toarray()\n",
    "    similarities = cosine_similarity(query_vector, X_tfidf_relevant.toarray()).flatten()\n",
    "    sorted_docs = [relevant_docs[i] for i in np.argsort(similarities)[::-1]]\n",
    "    sorted_similarities = np.sort(similarities)[::-1]\n",
    "    return sorted_docs, sorted_similarities\n",
    "\n",
    "# Solicitar la consulta del usuario\n",
    "query2 = \"the Bahia cocoa zone\"\n",
    "\n",
    "# Realizar la búsqueda utilizando BoW\n",
    "relevant_docs_bow, similarities_bow = search_bow(query2, vectorizer_bow, X_bow, inverted_index, preprocess_text)\n",
    "\n",
    "# Realizar la búsqueda utilizando TF-IDF\n",
    "relevant_docs_tfidf, similarities_tfidf = search_tfidf(query2, vectorizer_tfidf, X_tfidf, inverted_index, preprocess_text)\n",
    "\n",
    "# Mostrar resultados\n",
    "def display_results(relevant_docs, similarities, method_name):\n",
    "    print(f\"\\nDocumentos relevantes utilizando {method_name}:\")\n",
    "    num_docs_to_display = min(5, len(relevant_docs))\n",
    "    for i in range(num_docs_to_display):\n",
    "        doc_id = relevant_docs[i]\n",
    "        similarity = similarities[i]\n",
    "        print(f\"Documento {doc_id}, Similaridad: {similarity}\")\n",
    "\n",
    "display_results(relevant_docs_bow, similarities_bow, \"BoW\")\n",
    "display_results(relevant_docs_tfidf, similarities_tfidf, \"TF-IDF\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
