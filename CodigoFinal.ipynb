{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import SnowballStemmer \n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path de los datos \n",
    "#data_path = r'C:\\Users\\kevin\\OneDrive\\Documentos\\GitHub\\ProyectoRI\\data\\training_txt'\n",
    "#data_path = r'D:\\U\\7. Septimo\\RI\\ProyectoRI\\data\\training_txt'\n",
    "data_path = r'C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\training_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [] #  Vector de documentos\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith('.txt'): \n",
    "        path = os.path.join(data_path, filename) #Abrimos cada archivo \n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read() # leemos\n",
    "            documents.append((filename, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path de las stopwords\n",
    "#stopwords_path = r\"C:\\Users\\kevin\\OneDrive\\Documentos\\GitHub\\ProyectoRI\\data\\stopwords.txt\"\n",
    "#stopwords_path = r\"D:\\U\\7. Septimo\\RI\\ProyectoRI\\data\\stopwords.txt\"\n",
    "stopwords_path = r\"C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrimos el archivo \n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stop_words = set(file.read().splitlines()) # leemos las stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemmer se usa para reducir las palabras a su raíz\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una función que normaliza el texto con todos los requisitos necesarios:\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() #conviertimos en minúsculas\n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))#eliminamos los signos de puntuación\n",
    "    tokens = nltk.word_tokenize(text)#tokenizamos\n",
    "    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words] #aplicamos stemming\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = [(filename, preprocess_text(content)) for filename, content in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer solo los contenidos preprocesados para la vectorización\n",
    "preprocessed_contents = [content for _, content in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el vectorizador\n",
    "vectorizer_bow = CountVectorizer()\n",
    "# Vectorizar los contenidos preprocesados\n",
    "X_bow = vectorizer_bow.fit_transform(preprocessed_contents)\n",
    "feature_names_bow = vectorizer_bow.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor de busqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para convertir vectores a binarios\n",
    "def binary_vector(vector):\n",
    "    return (vector > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para calcular la similitud de Jaccard\n",
    "def jaccard_similarity(bin_vec1, bin_vec2):\n",
    "    intersection = np.sum(np.minimum(bin_vec1, bin_vec2))\n",
    "    union = np.sum(np.maximum(bin_vec1, bin_vec2))\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Definir la clase del motor de búsqueda\n",
    "class SearchEngine:\n",
    "    def __init__(self, vectorizer, document_vectors, documents, indices):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.document_vectors = document_vectors\n",
    "        self.documents = documents\n",
    "        self.indices = indices\n",
    "\n",
    "    def search(self, query):\n",
    "        query_vector = self.vectorizer.transform([query]).toarray()\n",
    "        bin_query_vector = binary_vector(query_vector)\n",
    "\n",
    "        scores = []\n",
    "        for i, doc_vector in enumerate(self.document_vectors):\n",
    "            bin_doc_vector = binary_vector(doc_vector.toarray())\n",
    "            similarity = jaccard_similarity(bin_query_vector, bin_doc_vector)\n",
    "            scores.append((similarity, self.indices[i]))\n",
    "        scores.sort(reverse=True, key=lambda x: x[0])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del motor de búsqueda\n",
    "documents = [doc for doc in preprocessed_contents]\n",
    "indices = [index for index, _ in preprocessed_documents]\n",
    "engine = SearchEngine(vectorizer_bow, X_bow, documents, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_documentos_relevantes(archivo):\n",
    "    documentos_relevantes = {}\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            partes = line.strip().split()\n",
    "            if len(partes) != 4:  # Asegurarse de que hay 4 partes en cada línea\n",
    "                continue  # Ignorar líneas que no siguen el formato esperado\n",
    "            numero_documento = partes[1][:-4]  # Obtener el número de documento eliminando la extensión .txt\n",
    "            try:\n",
    "                similitud = float(partes[-1])  # Convertir la similitud a un número decimal\n",
    "            except ValueError:\n",
    "                continue  # Ignorar líneas donde la similitud no es un número válido\n",
    "            documentos_relevantes[numero_documento] = similitud\n",
    "    return documentos_relevantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer documentos relevantes\n",
    "documentos_relevantes = leer_documentos_relevantes(r'C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\catslimpia.txt')\n",
    "#documentos_relevantes = leer_documentos_relevantes('C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\catslimpia.txt.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precision_recall(resultados_recuperados, resultados_relevantes):\n",
    "    num_resultados_recuperados = len(resultados_recuperados)\n",
    "    num_resultados_relevantes = len(resultados_relevantes)\n",
    "    \n",
    "    resultados_comunes = set(resultados_recuperados) & set(resultados_relevantes)\n",
    "    num_resultados_comunes = len(resultados_comunes)\n",
    "    \n",
    "    precision = num_resultados_comunes / num_resultados_recuperados if num_resultados_recuperados > 0 else 0\n",
    "    recall = num_resultados_comunes / num_resultados_relevantes if num_resultados_relevantes > 0 else 0\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_consultas(archivo):\n",
    "    consultas = []\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            consulta = line.strip()\n",
    "            consultas.append(consulta)\n",
    "    return consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer consultas\n",
    "ruta_archivo_consultas = r'C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\querys.txt'\n",
    "consultas = leer_consultas(ruta_archivo_consultas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_totales = {}  # Almacenar los resultados de todas las consultas\n",
    "for consulta in consultas:\n",
    "    resultados_recuperados = engine.search(consulta)\n",
    "    resultados_totales[consulta] = resultados_recuperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular precisión y recall para cada consulta\n",
    "precisiones = []\n",
    "recalls = []\n",
    "for consulta, documentos_recuperados in resultados_totales.items():\n",
    "    precision, recall = calcular_precision_recall(documentos_recuperados, documentos_relevantes)\n",
    "    precisiones.append(precision)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas de Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión promedio: 0.0\n",
      "Recall promedio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calcular la precisión y el recall promedio para todas las consultas\n",
    "precision_promedio = sum(precisiones) / len(precisiones)\n",
    "recall_promedio = sum(recalls) / len(recalls)\n",
    "\n",
    "print(\"Precisión promedio:\", precision_promedio)\n",
    "print(\"Recall promedio:\", recall_promedio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
