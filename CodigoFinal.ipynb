{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import SnowballStemmer \n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path de los datos \n",
    "#data_path = r'C:\\Users\\kevin\\OneDrive\\Documentos\\GitHub\\ProyectoRI\\data\\training_txt'\n",
    "data_path = r'D:\\U\\7. Septimo\\RI\\ProyectoRI\\data\\training_txt'\n",
    "#data_path = r'C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\training_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [] #  Vector de documentos\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith('.txt'): \n",
    "        path = os.path.join(data_path, filename) #Abrimos cada archivo \n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read() # leemos\n",
    "            documents.append((filename, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path de las stopwords\n",
    "#stopwords_path = r\"C:\\Users\\kevin\\OneDrive\\Documentos\\GitHub\\ProyectoRI\\data\\stopwords.txt\"\n",
    "stopwords_path = r\"D:\\U\\7. Septimo\\RI\\ProyectoRI\\data\\stopwords.txt\"\n",
    "#stopwords_path = r\"C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrimos el archivo \n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stop_words = set(file.read().splitlines()) # leemos las stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemmer se usa para reducir las palabras a su raíz\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una función que normaliza el texto con todos los requisitos necesarios:\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() #conviertimos en minúsculas\n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))#eliminamos los signos de puntuación\n",
    "    tokens = nltk.word_tokenize(text)#tokenizamos\n",
    "    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words] #aplicamos stemming\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = [(filename, preprocess_text(content)) for filename, content in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer solo los contenidos preprocesados para la vectorización\n",
    "preprocessed_contents = [content for _, content in preprocessed_documents]\n",
    "\n",
    "# Inicializar el vectorizador TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "# Vectorizar los contenidos preprocesados\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(preprocessed_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer solo los contenidos preprocesados para la vectorización\n",
    "preprocessed_contents = [content for _, content in preprocessed_documents]\n",
    "\n",
    "# Inicializar el vectorizador\n",
    "vectorizer_bow = CountVectorizer()\n",
    "\n",
    "# Vectorizar los contenidos preprocesados\n",
    "X_bow = vectorizer_bow.fit_transform(preprocessed_contents)\n",
    "feature_names_bow = vectorizer_bow.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path_cats = r'C:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\repoMantillaRI\\ProyectoRI\\data\\catslimpia.txt'\n",
    "#data_path_cats = r'C:\\Users\\kevin\\OneDrive\\Documentos\\GitHub\\ProyectoRI\\data\\catslimpia.txt'\n",
    "data_path_cats = r'D:\\U\\7. Septimo\\RI\\ProyectoRI\\data\\catslimpia.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar cats.txt\n",
    "cats = {}\n",
    "with open(data_path_cats, 'r') as f:\n",
    "    for line in f:\n",
    "        term, *docs = line.strip().split()\n",
    "        cats[term] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index_from_cat(cats):\n",
    "    inverted_index = {}\n",
    "    for term, docs in cats.items():\n",
    "        inverted_index[term] = set(docs)\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index_cats = build_inverted_index_from_cat(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_index_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexación BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(X, feature_names):\n",
    "    inverted_index = {}\n",
    "    for term_idx, term in enumerate(feature_names):\n",
    "        term_docs = set(X[:, term_idx].nonzero()[0])\n",
    "        inverted_index[term] = term_docs\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index_bow = build_inverted_index(X_bow, feature_names_bow)\n",
    "#inverted_index_tfidf = build_inverted_index(X_tfidf, feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor de busqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"japan revis longterm energi demand ministri\"\n",
    "# Vectorización de la consulta utilizando el vectorizador BoW\n",
    "query_vector_bow = vectorizer_bow.transform([query])\n",
    "\n",
    "# Calcular la similitud entre la consulta y los documentos utilizando BoW\n",
    "similarity_scores_bow = cosine_similarity(query_vector_bow, X_bow)\n",
    "\n",
    "# Obtener los documentos ordenados por similitud\n",
    "ranked_documents_bow = np.argsort(similarity_scores_bow)[0][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas de Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector_bow = vectorizer_bow.transform([\" \".join(cats.keys())])\n",
    "similarity_scores_bow = cosine_similarity(query_vector_bow, X_bow)\n",
    "ranked_documents_bow = np.argsort(similarity_scores_bow)[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión y recall para BoW:\n",
      "Precisión: 0.0\n",
      "Recall: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"\\nPrecisión y recall para TF-IDF:\")\\nprint(\"Precisión:\", precision_tfidf)\\nprint(\"Recall:\", recall_tfidf)\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relevant_documents_for_query(query_terms, index):\n",
    "    relevant_docs = set()\n",
    "    for term in query_terms:\n",
    "        if term in index:\n",
    "            relevant_docs.update(index[term])\n",
    "    return relevant_docs\n",
    "\n",
    "query_terms = list(cats.keys())  # Lista de términos de consulta a partir de las categorías\n",
    "\n",
    "# Obtener documentos relevantes para la consulta actual\n",
    "relevant_docs_bow = relevant_documents_for_query(query_terms, inverted_index_cats)\n",
    "relevant_docs_tfidf = relevant_documents_for_query(query_terms, inverted_index_cats)\n",
    "\n",
    "# Calcular precisión y recall\n",
    "def precision_recall(ranked_documents, relevant_docs, total_documents):\n",
    "    retrieved_docs = [doc for doc in ranked_documents if doc in relevant_docs]\n",
    "    TP = len(retrieved_docs)\n",
    "    FP = len(ranked_documents) - TP\n",
    "    FN = len(relevant_docs) - TP\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "total_documents = X_bow.shape[0]\n",
    "\n",
    "# Calcular precisión y recall para BoW\n",
    "precision_bow, recall_bow = precision_recall(ranked_documents_bow, relevant_docs_bow, total_documents)\n",
    "\n",
    "print(\"Precisión y recall para BoW:\")\n",
    "print(\"Precisión:\", precision_bow)\n",
    "print(\"Recall:\", recall_bow)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
